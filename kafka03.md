## 카프카의 기본 구성  
![](https://velog.velcdn.com/images/dmg919/post/c6e9a13d-5e32-4026-9850-0b44f8b46c42/image.png)
카프카 애플리케이션, 주키퍼 애플리케이션, 총 2개의 애플리케이션을 설치하고 운영  
브로커 = 카프카 애플리케이션이 설치된 서버 또는 노드  
## 리플리케이션
리플리케이션 : 각 메시지들을 여러 개로 복제해서 카프카 클러스터 내 브로커들에 분산시키는 동작  
리플리케이션 동작 덕분에 하나의 브로커가 종료되더라도 카프카는 안정성을 유지할 수 있다.  
replication-factor 옵션은 카프카 내 몇 개의 리플리케이션을 유지할지 정함
ex) replication-factor가 3이면 원본을 포함한 리플리케이션이 총 3개가 있다는 뜻이다.
## 파티션
![](https://velog.velcdn.com/images/dmg919/post/43e7e0e4-0399-47d9-911c-d2300bb3fc3f/image.png)
- 토픽1은 파티션 0이라는 1개의 파티션으로 구성
- 토픽 3은 파티션0, 파티션1, 파티션2로 총 3개의 파티션으로 구성
---
- 파티션 수는 초기 생성 후 언제든지 늘릴 수 있지만, 반대로 한 번 늘리면 절대로 줄일 수 없음
- 작게 생성해서 늘려가는 방식이 바람직함 (메시지 처리량, 컨슈머의 LAG 등)
컨슈머의 LAG = 프로듀서가 보낸 메시지 수(카프카에 남아 있는 메시지 수) - 컨슈머가 가져간 메시지 수
-> 컨슈머에 지연 여부 확인
## 세그먼트
프로듀서에 의해 브로커로 전송된 메시지는 토픽의 파티션에 저장되며, 각 메시지들은 로그 파일의 형태로 브로커의 로컬 디스크에 저장된다.
![](https://velog.velcdn.com/images/dmg919/post/a66bc90d-de5c-4b31-90d9-9cb2e85c764d/image.png)
각 파티션마다 N개의 세그먼트 로그 파일들이 존재한다.
![](https://velog.velcdn.com/images/dmg919/post/41373322-af1e-4d24-bcc6-5807dd1c93cf/image.png)
프로듀서에서 peter-overview01 토픽으로 메시지를 전송하고 파티션0의 세그먼트 로그 파일에 저장한다. 저장된 메시지는 컨슈머가 읽어갈 수 있다.
## 분산 시스템
- 분산 시스템 : 네트워크 상에서 연결된 컴퓨터들의 그룹, 단일 시스템이 갖지 못한 높은 성능
- 장점
	- 서버 또는 노드 등에 장애가 발생할 때 다른 서버 또는 노드가 대신 처리하므로 장애 대응이 탁월
	- 부하가 높은 경우 시스템 확장이 용이
-> 카프카는 브로커를 추가하는 방식으로 확장 가능
## 페이지 캐시
페이지 캐시 : 직접 디스크에 읽고 쓰는 대신 물리 메모리 중 애플리케이션이 사용하지 않는 일부 잔여 메모리 활용
카프카는 OS의 페이지 캐시를 활용하는 방식 -> 직접 디스크에서 읽고 쓰지 않고 페이지 캐시를 통해 읽고 쓰기를 한다.
![](https://velog.velcdn.com/images/dmg919/post/ae8a56b5-8de3-4f78-bb40-cc9fac74534c/image.png)
## 배치 전송 처리
수많은 통신을 묶어서 처리하여 네트워크 오버헤드를 줄이고 장기적으로 더욱 빠르고 효율적인 처리
EX) 상품 재고 수량 업데이트 작업 -> 실시간 처리 / 구매 로그를 저장소로 보내는 작업 -> 배치 처리
## 압축 전송
네트워크 대역폭이나 회선 비용을 줄일 수 있다.
- 높은 압축률 : gzip, zstd 권장 / 빠른 응답속도 : lz4, snappy 권장
## 토픽, 파티션, 오프셋
- 토픽 : 이메일 전송 시스템에서 이메일 주소 정도의 개념
- 파티션 : 토픽의 병렬 처리를 위해 나눠진 단위, 높은 처리량 수행
- 오프셋(offset) : 순차적으로 증가하는 숫자 형태, 파티션에서의 고유한 숫자, 메시지의 순서 보장, 컨슈머에서 마지막까지 읽은 위치 알 수 있음
## 고가용성 보장
- 리플리케이션 기능은 토픽 자체 복제가 아닌 파티션을 복제
- 원본과 리플리케이션을 구분하기 위해 리더(leader)와 팔로워(follower)라 부름
- 팔로워의 수만큼 브로커의 디스크 공간도 소비되므로 이상적인 리플리케이션 팩터 수를 유지, 일반적으로 카프카에서는 리플리케이션 팩터 수를 3으로 구성하도록 권장한다.
- 리터 : 프로듀서, 컨슈머로부터 오는 모든 읽기와 쓰기 요청 처리
- 팔로워 : 오직 리더로부터 리플리케이션하게 된다.
## 주키퍼의 의존성
- 주키퍼는 여러 대의 서버를 앙상블(클러스터)로 구성하고, 살아있는 노드 수가 과반수 이상 유지된다면 지속적인 서비스가 가능한 구조, 따라서 반드시 홀수로 구성해야 한다.
- 지노드(znode)를 이용해 **브로커의 노드관리**, **메타 정보   기록**, 토픽 관리, 컨트롤러 관리 등의 중요한 역할을 하고 있다.
## 프로듀서
![](https://velog.velcdn.com/images/dmg919/post/517d7f0e-58cd-43ad-af82-66420bfbcd22/image.png)
레코드에서 토픽과 밸류(메시지 내용)은 필숫값이며, 특정 파티션을 지정하기 위한 레코드의 파티션과 특정 파티션에 레코드들을 정렬하기 위한 레코드의 키는 선택사항(옵션)이다.

1. 프로듀서가 카프카로 레코드를 전송할 때, 카프카의 특정 토픽으로 메시지를 전송한다.
2. 각 레코드들은 프로듀서의 send() 메서드를 통해 serializer, partitioner를 거친다.
3. (1) 파티셔너를 지정했다면 파티셔너는 아무 동작도 하지 않고 지정된 파티션으로 레코드를 전달한다.
(2) 파티션을 지정하지 않은 경우 키를 가지고 파티션을 선택해 레코드를 전달하는데 기본적으로 라운드 로빈 방식으로 동작한다.
4. 프로듀서 내부에서는 send() 메서드 동작 이후 레코드들을 파티션별로 잠시 모아둔다. (배치 전송)
5. 전송이 실패하면 재시도 동작이 이뤄지고, 지정된 횟수만큼 재시도가 실패하면 최종 실패를 전달하며, 전송이 성공하면 메타데이터를 리턴하게 된다.
## 컨슈머
- 우리는 컨슈머를 이용해 토픽에 저장된 메시지를 가져올 수 있다.
- 컨슈머 그룹은 하나 이상의 컨슈머들이 모여 있는 그룹을 의미하고, 컨슈머는 반드시 컨슈머 그룹에 속하게 된다.
- 이 컨슈머 그룹은 각 파티션의 리더에게 카프카 토픽에 저장된 메시지를 가져오기 위한 요청을 보낸다.
- 파티션 수와 컨슈머 수는 일대일로 매핑되는 것이 이상적이다.
## 컨슈머 그룹
![](https://velog.velcdn.com/images/dmg919/post/fc9d4d5e-405b-4195-a86d-25e3a0339c0b/image.png)
- 컨슈머들은 토픽의 파티션과 일대일로 매핑되어 메시지를 가져오게 된다.
- 그룹 내 컨슈머들은 서로의 정보를 공유한다.
예를 들어 컨슈머01이 문제가 생겨 종료됐다면, 컨슈머02 또는 컨슈머03은 컨슈머01이 하던 일을 대신해 Topic-01 토픽의 파티션0을 컨슘하기 시작한다.
## 출처
